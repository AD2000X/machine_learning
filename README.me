# Machine Learning and Deep Learning Models

## Table of Contents
- [Machine Learning Models](#machine-learning-models)
  - [Linear Regression](#1-linear-regression)
  - [Stochastic Gradient Descent (SGD) Regressor](#2-stochastic-gradient-descent-sgd-regressor)
  - [Polynomial Regression](#3-polynomial-regression)
  - [Decision Tree](#4-decision-tree)
  - [K-Nearest Neighbors (KNN)](#5-k-nearest-neighbors-knn)
  - [Random Forest](#6-random-forest)
  - [Gradient Boosting](#7-gradient-boosting)
  - [Adaptive Boosting (AdaBoost)](#8-adaptive-boosting-adaboost)
  - [Support Vector Machine (SVM)](#9-support-vector-machine-svm)
  - [Logistic Regression](#10-logistic-regression)
- [Deep Learning](#deep-learning)
  - [Multi-Layer Perceptron (MLP)](#1-multi-layer-perceptron-mlp)
- [Shared Techniques Across Models](#shared-techniques-across-models)
  - [Model Training and Evaluation](#model-training-and-evaluation)
  - [Hyperparameter Tuning](#hyperparameter-tuning)

---

## Machine Learning Models

### 1. Linear Regression
- **Library**: `sklearn.linear_model.LinearRegression`
- **Purpose**: Models the linear relationship between features and target for regression problems.
- **Usage**: Predicting continuous variables (e.g., body temperature).

### 2. Stochastic Gradient Descent (SGD) Regressor
- **Library**: `sklearn.linear_model.SGDRegressor`
- **Purpose**: Optimizes linear models using stochastic gradient descent, suitable for large datasets.

### 3. Polynomial Regression
- **Library**: `sklearn.preprocessing.PolynomialFeatures` combined with `LinearRegression`
- **Purpose**: Captures non-linear relationships by expanding features to higher-degree terms.
- **Implementation**: Used within a pipeline (`make_pipeline`).

### 4. Decision Tree
- **Libraries**:
  - `sklearn.tree.DecisionTreeRegressor` (for regression)
  - `sklearn.tree.DecisionTreeClassifier` (for classification)
- **Purpose**: Constructs tree structures to make predictions for regression and classification tasks.

### 5. K-Nearest Neighbors (KNN)
- **Library**: `sklearn.neighbors.KNeighborsRegressor`
- **Purpose**: Predicts outcomes based on the nearest data points in the feature space.
- **Customization**: Experimented with different values of `k` (e.g., `k=1` and `k=14`).

### 6. Random Forest
- **Libraries**:
  - `sklearn.ensemble.RandomForestRegressor` (for regression)
  - `sklearn.ensemble.RandomForestClassifier` (for classification)
- **Purpose**: Combines predictions from multiple decision trees to improve generalization.

### 7. Gradient Boosting
- **Library**: `sklearn.ensemble.GradientBoostingClassifier`
- **Purpose**: Sequentially builds decision trees to minimize prediction errors iteratively.

### 8. Adaptive Boosting (AdaBoost)
- **Library**: `sklearn.ensemble.AdaBoostClassifier`
- **Purpose**: Focuses on difficult samples by adjusting weights in each iteration.

### 9. Support Vector Machine (SVM)
- **Library**: `sklearn.svm.SVC`
- **Purpose**: Finds the optimal hyperplane for classification, effective in high-dimensional spaces.

### 10. Logistic Regression
- **Library**: `sklearn.linear_model.LogisticRegression`
- **Purpose**: Solves binary classification problems using a probabilistic approach with sigmoid functions.

---

## Deep Learning

### 1. Multi-Layer Perceptron (MLP)
- **Library**: `tensorflow.keras`
- **Implementation**:
  - **Architecture**: Sequential model with input, hidden, and output layers.
  - **Regularization**: L1 and L2 penalties to avoid overfitting.
  - **Early stopping**: Improves generalization.
- **Purpose**: Non-linear regression and classification tasks using neural networks.

---

## Shared Techniques Across Models

### Model Training and Evaluation
- **Techniques**:
  - Models are trained, validated, and evaluated using performance metrics such as RMSE and F1 score.
  - Cross-validation techniques are applied to assess generalization.

### Hyperparameter Tuning
- **Methods**:
  - Grid Search (`GridSearchCV`) and Randomized Search (`RandomizedSearchCV`) for optimizing model parameters.
